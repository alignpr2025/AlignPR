prompt,repopath,filepath,hitPath,file,startLine,endLine,summary
Fine me the code where it merges front end and back end data,hyperion,"hyperion
/2408.06037v1.pdf",hyperion/main.py,hyperion/main.py,60,65,"The code here merges frontend and backend data by calling the Semantics class with platform, address, and block number parameters, then obtaining inputs from semantic.get_inputs()[0] which is used in symbolic_execution.ir_se.run()"
find me the code where it trains and fine tunes a large language model,hyperion,"hyperion
/2408.06037v1.pdf",hyperion/main.py,hyperion/main.py,60,119,"The code trains and fine-tunes a large language model in the run method of the Semantics class, which is used for symbolic execution and analysis."
fine me the code where it defines seven inconsistencies between front end and back end,hyperion,"hyperion
/2408.06037v1.pdf",hyperion/main.py,hyperion/main.py,60,61,The code defines seven inconsistencies between front end and back end in the global configuration variables.
find me the code where it preprocesses dataset to concatenate them,hyperion,"hyperion
/2408.06037v1.pdf",hyperion/main.py,hyperion/main.py,120,145,Preprocessing of the dataset is done to concatenate them in order to perform analysis and extraction on this combined data. This is shown in a block of code where a list of inputs is converted into a format suitable for semantic analysis.
find me the code where loads some saftey models to check safety,hyperion,"hyperion
/2408.06037v1.pdf",hyperion/main.py,hyperion/main.py,120,145,load safety models to check safety
Find me how does it get the commit context using the url,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,60,120,"git_diff_tool = Tool(name\u003d""Git diff collector tool""
                , func\u003dget_git_diff_from_commit_url,
                description\u003d""Given a commit url, this tool outputs git diff. Input should be a full commit url."""
Fine me the code where it counts collected issues and prs,omega,omega/omega_paper.pdf,,,0,15,
find me the code where it defines quantization for models,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,0,242,quantization for models is not defined here
find me the code where it extracts the commit using github url,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,239,240,get_commit_from_github(commit_url) is used to extract the commit using github url
Find me the code where it constructs the after summary of a commit,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,120,239,"The after summary of a commit is constructed in the ImportantFileTool class, which ranks changed files by their importance."
fine me the code where it defines zero shot prompt,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,60,119,"diff_explanation_method == ""zeroshot"""
fine me the code where it gets the pull request content based on using github url,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,180,239,get_pull_request_content is used in this class: PullRequestCollectingTool
"fine me the code where it is implemented-""Specifically, for a
modified method, we first generated the pre-commit multiintent summaries""",omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,60,64,generate_multi_intent_summaries function is implemented in this code region.
where did the java projects download,omega,omega/omega_paper.pdf,omega/CMG/Agent_tools.py,omega/CMG/Agent_tools.py,10,23,get_git_diff_from_commit_url function to download Java projects
where does it calculate file level result,sound,omega/sound_paper.pdf,,,0,15,
find me the code where it ranks analysis from file and line level,sound,omega/sound_paper.pdf,,,0,15,
find me the code where it runs analysis to answer RQ1,sound,omega/sound_paper.pdf,,,0,15,
where does it select the top 100 releases,sound,omega/sound_paper.pdf,,,0,15,
fine me the code where it calculates recall for buggy lines',sound,omega/sound_paper.pdf,,,0,15,
find me the code where it reads the dataset line by line and file by file,sound,omega/sound_paper.pdf,,,0,15,
where does it calculate cohen's kapps between two authors,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/classification_result_reader_writer.py,hf-question-answer-main/discussion_classifier/classification_result_reader_writer.py,37,38,Calculates Cohen's Kappa between two authors when get_final_class() function is executed
where does it rank model based on likes,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/classification_result_reader_writer.py,hf-question-answer-main/discussion_classifier/classification_result_reader_writer.py,34,41,"get_final_class function calculates the final class of a discussion based on its run 1 and run 2 classes, and ties are broken by tie breaking class"
where does it calculate percentile for discussion length?,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/gpt_classifier.py,hf-question-answer-main/discussion_classifier/gpt_classifier.py,41,42, percentile calculation in get_final_class function
where does it implement finding random discussion length removing urls and hyperlinks,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/gpt_classifier.py,hf-question-answer-main/discussion_classifier/gpt_classifier.py,15,17,"This function is where the GPT classifier works, and it involves discussing random discussion length removing URLs and hyperlinks."
find me the code where it downloads discussions for models,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/gpt_classifier.py,hf-question-answer-main/discussion_classifier/gpt_classifier.py,15,17,downloading discussions for models in this region of code
find me the code where it downloads all models from hugging face,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/gpt_classifier.py,hf-question-answer-main/discussion_classifier/gpt_classifier.py,26,27,"request_to_gpt(client: OpenAI, discussion: Discussion) function calls the openai.completions.create endpoint to interact with the GPT-3.5 model."
find me the code where it classify discussions using open ai key,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/gpt_classifier.py,hf-question-answer-main/discussion_classifier/gpt_classifier.py,60,79,where it classify discussions using open ai key
where does it implement identifying hidden discussions,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/gpt_classifier.py,hf-question-answer-main/discussion_classifier/gpt_classifier.py,28,33,classifying_discussions function
where does it clean all discussions and save them,hf-question-answer-main,ICSE_2025_Toma.pdf,hf-question-answer-main/discussion_classifier/all_discussion_classifier.py [60-79],hf-question-answer-main/discussion_classifier/all_discussion_classifier.py [60-79],68,69,save_cleaned_all_discussions(discussions)
find the loss function used in this study,C4RLLaMA,idllm.pdf,C4RLLaMA/train.py,C4RLLaMA/train.py,61,65,"The loss function is not explicitly stated in the code. However, based on the implementation of the train() function and the structure of a typical PyTorch training setup, it can be inferred that the loss function being used is CrossEntropyLoss or one of its variants."
where does it declare the smoothing loss function,C4RLLaMA,idllm.pdf,C4RLLaMA/train.py,C4RLLaMA/train.py,63,67,def SmoothingTrainingArguments(transformers.TrainingArguments):
find the zero shot prompting strategy in the codbase,C4RLLaMA,idllm.pdf,C4RLLaMA/test.py,C4RLLaMA/test.py,80,81,zero-shot prompting strategy is used in the evaluate function
find where it calculates precision,C4RLLaMA,idllm.pdf,C4RLLaMA/test.py,C4RLLaMA/test.py,53,56,computes precision in compute function
find the base model this codebase uses,C4RLLaMA,idllm.pdf,C4RLLaMA/test.py,C4RLLaMA/test.py,120,154,"""huggyllama/llama-7b"""
find the code fragment where it produces results,C4RLLaMA,idllm.pdf,C4RLLaMA/test.py,C4RLLaMA/test.py,124,143,produce results
find the code where it loads the pretrained model,C4RLLaMA,idllm.pdf,C4RLLaMA/test.py,C4RLLaMA/test.py,71,73,loads the pre-trained model using AutoModelForCausalLM.from_pretrained
find the tokenize function,C4RLLaMA,idllm.pdf,C4RLLaMA/train.py,C4RLLaMA/train.py,60,68,define tokenize function
find the configuration for low rank adaptation,C4RLLaMA,idllm.pdf,C4RLLaMA/train.py,C4RLLaMA/train.py,93,108,lora_r parameter configuration
what optimizer and scheduler did it use? find the relevant code fragment,C4RLLaMA,idllm.pdf,C4RLLaMA/train.py,C4RLLaMA/train.py,60,64,Schedulers are set when calling the fit method of a PyTorch Trainer object.
which scripts regenerate profiling/anomaly metrics and log the reported precision/recall/F1 per dataset?,riolu,2412.05240v1.pdf,riolu/Auto-RIOLU.py,riolu/Auto-RIOLU.py,61,121,Calculates precision/recall/F1 for each dataset and logs the results
where does it use the code for  2 means clustering techniques described in the paper,riolu,2412.05240v1.pdf,riolu/Guided-RIOLU.py,riolu/Guided-RIOLU.py,63,80,Cluster analysis using KMeans from scikit-learn: \n    from sklearn.cluster import KMeans \nclass PatternGenerator:
Does it check whether data is cleaned or not? if yes find me the relevant code where it is implemented?,riolu,2412.05240v1.pdf,riolu/Guided-RIOLU.py,riolu/Guided-RIOLU.py,63,76,The code checks whether data is cleaned or not by comparing the original dirty data with the cleaned data in the Guided-RIOLU.py file.
how can i estimate the coverage given a threshold for AUTO RIOLU?,riolu,2412.05240v1.pdf,riolu/Auto-RIOLU.py,riolu/Auto-RIOLU.py,60,88,The code is estimating the coverage given a threshold for AUTO RIOLU. It fine tunes the coverage threshold in the function coverage_estimation.
where to get the results of supervised version of rolu?,riolu,2412.05240v1.pdf,riolu/Auto-RIOLU.py,riolu/Auto-RIOLU.py,60,88,"def information_gathering(self, symbols, column, coverage):"
where to find the actual coverage of the pattern,riolu,2412.05240v1.pdf,riolu/Auto-RIOLU.py,riolu/Auto-RIOLU.py,62,83,Coverage estimation is performed to fine tune the coverage threshold
How do I run DetectCodeGPT end-to-end on my own code files?,DetectCodeGPT-main,2401.06461v5.pdf,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,10,24,Mapping the function that runs the DetectCodeGPT end-to-end on my own code files.
What default hyperparameters should I use for perturbations,DetectCodeGPT-main,2401.06461v5.pdf,,,0,15,
where does it generate code using hugging face code parrot models ?,DetectCodeGPT-main,2401.06461v5.pdf,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,266,274,"Mapping the question to the most relevant code region, this function align_tokens_with_categories seems to be related to Hugging Face's tokenizer and tree-sitter. It appears to be used for analyzing code pairs."
how the codebase implement the naturalness distribution part? find me the relevant code part,DetectCodeGPT-main,2401.06461v5.pdf,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,251,262,"Mapping the naturalness distribution part, searching for related code regions."
how does the paper mention finding tagging tokens and aligning the token with categories,DetectCodeGPT-main,2401.06461v5.pdf,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,DetectCodeGPT-main/code-analysis/analyze_law_and_frequency.py,248,254,"function align_tokens_with_categories(code, lang, tokenizer, return_hf_tokens=False):"
where does it train linear discrimant analysis model find me the relevant code part,DESEC-main,2410.08858v2.pdf,DESEC-main/get_prompt.py,DESEC-main/get_prompt.py,32,45,find_linear_discriminant_analysis_model
where does it calculate shannon entropy for evaluating secrets?,DESEC-main,2410.08858v2.pdf,DESEC-main/Decoding/DESEC_BeamSearch.py,DESEC-main/Decoding/DESEC_BeamSearch.py,100,107,calculates shannon entropy for evaluating secrets
beam search for decoding?,DESEC-main,2410.08858v2.pdf,DESEC-main/Decoding/DESEC_BeamSearch.py,DESEC-main/Decoding/DESEC_BeamSearch.py,67,109,beam search algorithm implementation in DESEC project
average entropy calculation?,DESEC-main,2410.08858v2.pdf,DESEC-main/Decoding/DESEC_BeamSearch.py,DESEC-main/Decoding/DESEC_BeamSearch.py,67,78,Entropy after adding this token new_entropies = entropy_by_token(new_de_tokens)
how does it crawl secrets from github?,DESEC-main,2410.08858v2.pdf,DESEC-main/Decoding/DESEC_TestWithCodeLLM.py,DESEC-main/Decoding/DESEC_TestWithCodeLLM.py,54,55,"'secret_name': [Prefix(If not, it is empty), Secret's text length, Token Constraint]"
it has mentioned user can compare DESEC with chosen LLM? where is the relevant part?,DESEC-main,2410.08858v2.pdf,DESEC-main/Decoding/DESEC_TestWithCodeLLM.py,DESEC-main/Decoding/DESEC_TestWithCodeLLM.py,40,41,the code has a section that compares DESEC with chosen LLM
where does it insert relations for graph nodes mentioned in the paper?,fse2025/ProAlert-main,fse2025/3729367.pdf,fse2025/ProAlert-main/pro_alert/summarize.py,fse2025/ProAlert-main/pro_alert/summarize.py,240,299,inserting relations for graph nodes mentioned in the paper
where does it preprocess and prepare its data?,fse2025/ProAlert-main,fse2025/3729367.pdf,fse2025/ProAlert-main/pro_alert/summarize.py,fse2025/ProAlert-main/pro_alert/summarize.py,240,241,preprocess and prepare its data in function: prepare_input_data
how does it read the topology? which graph algorithms does it use?,fse2025/ProAlert-main,fse2025/3729367.pdf,fse2025/ProAlert-main/pro_alert/summarize.py,fse2025/ProAlert-main/pro_alert/summarize.py,240,299,Reading the topology is done in this file using graph_query function
how does it cluster the anomalies found?,fse2025/ProAlert-main,fse2025/3729367.pdf,fse2025/ProAlert-main/pro_alert/summarize.py,fse2025/ProAlert-main/pro_alert/summarize.py,240,299,clustering the anomalies found through read_label_pair_features function
where the codebase propagates to next k hops in graph,fse2025/ProAlert-main,fse2025/3729367.pdf,fse2025/ProAlert-main/pro_alert/summarize.py,fse2025/ProAlert-main/pro_alert/summarize.py,258,260,propagates to next k hops in graph
what are inputs that the tool take?,fse2025/BugSpot-Artifact-master,fse2025/3729370.pdf,fse2025/BugSpot-Artifact-master/tool_main.py,fse2025/BugSpot-Artifact-master/tool_main.py,17,21,"The tool takes the following inputs: report_file, output_dir, device_id, app_pkg, dsl_file_path, logger_level, use_gpt_4, use_gpt_4_turbo, crawl_info."
does it use any llm models? if yes find me the code where it specifies the models?,fse2025/BugSpot-Artifact-master,fse2025/3729370.pdf,fse2025/BugSpot-Artifact-master/report_parser/parser_main.py,fse2025/BugSpot-Artifact-master/report_parser/parser_main.py,11,18,"This is where the llm_query function is defined, which uses LLM models for processing."
how do they check if the bug report has been reproduced successfully?,fse2025/BugSpot-Artifact-master,fse2025/3729370.pdf,fse2025/BugSpot-Artifact-master/tool_main.py,fse2025/BugSpot-Artifact-master/tool_main.py,60,72,Reproduction Results: Success | Failure
how does it check if the information of the bug report is correct?,fse2025/BugSpot-Artifact-master,fse2025/3729370.pdf,fse2025/BugSpot-Artifact-master/tool_main.py,fse2025/BugSpot-Artifact-master/tool_main.py,60,72,It checks if the information of the bug report is correct by verifying the DSL on the app info via dsl_inspector.
how does it load all the buggy reports and prompts the model?,fse2025/BugSpot-Artifact-master,fse2025/3729370.pdf,fse2025/BugSpot-Artifact-master/report_parser/parser_main.py,fse2025/BugSpot-Artifact-master/report_parser/parser_main.py,10,15,The function `load_prompts` loads all the buggy reports and prompts the model in the line: return prompt_text
how does it cluster depending on a threshold?,fse2025/trustworthiness-oracle,fse2025/2410.22663v4.pdf,fse2025/trustworthiness-oracle/src/calculate_score.py,fse2025/trustworthiness-oracle/src/calculate_score.py,22,23,The function do_consider_word uses a threshold (options['threshold_lime']) to determine if the lime score is sufficient
where does it load all of datasets for classifiers?,fse2025/trustworthiness-oracle,fse2025/2410.22663v4.pdf,fse2025/trustworthiness-oracle/src/attack_main.py,fse2025/trustworthiness-oracle/src/attack_main.py,180,181,loads all datasets for classifiers in the line where it sets the ensemble variable
how does it produce the output from inputs to use later on for proving trustworthiness,fse2025/trustworthiness-oracle,fse2025/2410.22663v4.pdf,fse2025/trustworthiness-oracle/src/attack_main.py,fse2025/trustworthiness-oracle/src/attack_main.py,59,60,load_toki function initializes the TOKI class with various datasets and embeddings.
how does it evaluate whether words are similar from semantic point of view?,fse2025/trustworthiness-oracle,fse2025/2410.22663v4.pdf,fse2025/trustworthiness-oracle/src/attack_main.py,fse2025/trustworthiness-oracle/src/attack_main.py,60,61,"The code snippet defines the do_consider_word function which evaluates whether words are similar from a semantic point of view by checking the Lime score, confidence, and other conditions."
"where does it load all explanations, input, outputs from the model for test training purpose?",fse2025/trustworthiness-oracle,fse2025/2410.22663v4.pdf,,,0,15,
how does it preprocess and read the codes from the path?,fse2025/testsmellrefactoring-master,fse2025/2409.16739v2.pdf,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,0,59,preprocess and read the codes from the path
it says it builds a prompt in purpose of refactoring for llm model?,fse2025/testsmellrefactoring-master,fse2025/2409.16739v2.pdf,fse2025/testsmellrefactoring-master/refactoring/buildindex/code_repo_v5.py,fse2025/testsmellrefactoring-master/refactoring/buildindex/code_repo_v5.py,128,151,"build prompt in purpose of refactoring for llm model: build_prompt_total test_code, context, smell_list, smell_desc_list, refactoring_dsl, smell_refactoring_checkpoint_list"
find me the started code for test refactoring,fse2025/testsmellrefactoring-master,fse2025/2409.16739v2.pdf,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,0,59,read_file_content (testsmell.loading.process) function is first code in file
where does it define all the smell type for test refactoring?,fse2025/testsmellrefactoring-master,fse2025/2409.16739v2.pdf,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,67,79,it defines all the smell type for test refactoring in this section
where does it process all the smells detected?,fse2025/testsmellrefactoring-master,fse2025/2409.16739v2.pdf,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,fse2025/testsmellrefactoring-master/refactoring/refactoring_test.py,83,89,Processing of smells detected in the smells list
where does it rescale the logits?,fse2025/CalibrationLLMsCodeSummaries-main,fse2025/2404.19318v3.pdf,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,66,66,where does it rescale the logits?
where does it generate llm rating ?,fse2025/CalibrationLLMsCodeSummaries-main,fse2025/2404.19318v3.pdf,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,108,109,"def brier_score_significance_testing(errors_1, errors_2): defines a function to perform significance testing on the Brier scores using the paired t-test."
find me the code where it uses similairity score and model logits to plot similarity?,fse2025/CalibrationLLMsCodeSummaries-main,fse2025/2404.19318v3.pdf,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,60,119,"calculate_calibration_metrics predict the probabilities of labels and calculate Brier score, which is a measure of similarity between predicted and true probabilities."
where does it declare all the llm models to infer?,fse2025/CalibrationLLMsCodeSummaries-main,fse2025/2404.19318v3.pdf,fse2025/CalibrationLLMsCodeSummaries-main/benchmarking.py,fse2025/CalibrationLLMsCodeSummaries-main/benchmarking.py,6,42,Declares all the LLM models to infer in the line: 'model_files = [...]'.
find me the code where it calculates metrics?,fse2025/CalibrationLLMsCodeSummaries-main,fse2025/2404.19318v3.pdf,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,62,119,"Calculates various metrics such as ECE, Brier score, Skill score using the given predicted probabilities and true labels."
where does it calculate calibration metrics?,fse2025/CalibrationLLMsCodeSummaries-main,fse2025/2404.19318v3.pdf,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,fse2025/CalibrationLLMsCodeSummaries-main/calibration_metrics.py,67,76,calculate_calibration_metrics function calculates calibration metrics
it says it uses token position in prompt? can you find the relevant code ?,fse2025/contextcraft-main,fse2025/3715753.pdf,fse2025/contextcraft-main/Source_Code/ChatGPT/chatgpt4_config.py,fse2025/contextcraft-main/Source_Code/ChatGPT/chatgpt4_config.py,7,16,"This file contains the ""get_chatgpt4_response"" method which sends a prompt to the ChatGPT-4 model and retrieves the response, which may use token position in prompt."
where is the main file for method name prediction using the prompts as mentioned in the paper?,fse2025/contextcraft-main,fse2025/3715753.pdf,fse2025/contextcraft-main/Source_Code/ChatGPT/main.py,fse2025/contextcraft-main/Source_Code/ChatGPT/main.py,3,17,The main file for method name prediction using the prompts as mentioned in the paper is main.py.
find me the configuration for the gemini api usage?,fse2025/contextcraft-main,fse2025/3715753.pdf,fse2025/contextcraft-main/Source_Code/ChatGPT/chatgpt4_config.py,fse2025/contextcraft-main/Source_Code/ChatGPT/chatgpt4_config.py,7,9,The configuration for the gemini api usage is defined in this file.
in the paper it mentions it finds the highest similarity between the description and function name?,fse2025/contextcraft-main,fse2025/3715753.pdf,fse2025/contextcraft-main/Source_Code/ChatGPT/chatgpt4_config.py,fse2025/contextcraft-main/Source_Code/ChatGPT/chatgpt4_config.py,10,11,The description in the paper mentions finding the highest similarity between the description and function name. This is relevant to the get_chatgpt4_response method in chatgpt4_config.py
how does it calculate probability for suffixes prefixes of the token?,fse2025/contextcraft-main,fse2025/3715753.pdf,fse2025/contextcraft-main/Source_Code/ChatGPT/text_processing.py,fse2025/contextcraft-main/Source_Code/ChatGPT/text_processing.py,0,6,Tokenization and Language Modeling for Suffixes and Prefixes
where does it evaluate repository and calculate time taken?,ase2025/FastCoder-main,ase2025/2502.17139v2.pdf,ase2025/FastCoder-main/datastore/get_repo_datastore.py,ase2025/FastCoder-main/datastore/get_repo_datastore.py,60,97,where does it evaluate repository and calculate time taken?
how they train the model?,ase2025/FastCoder-main,ase2025/2502.17139v2.pdf,ase2025/FastCoder-main/datastore/get_common_datastore.py,ase2025/FastCoder-main/datastore/get_common_datastore.py,0,43,The code snippet for loading and processing a dataset using DraftRetriever is located here.
what is the code where they implement the caching of previous states ?,ase2025/FastCoder-main,ase2025/2502.17139v2.pdf,ase2025/FastCoder-main/evaluation/autoregressive_test.py,ase2025/FastCoder-main/evaluation/autoregressive_test.py,64,69,cache initialization for past key and value states
how do they evaluate the best candidate depending on the prevoious states?,ase2025/FastCoder-main,ase2025/2502.17139v2.pdf,ase2025/FastCoder-main/datastore/get_common_datastore.py,ase2025/FastCoder-main/datastore/get_common_datastore.py,10,43,This code retrieves common datastores from the Hugging Face dataset and creates an index file for tokenization. The model path is parsed as an argument to load the tokenizer and config.
how do they generate the candidates for the outputs?,ase2025/FastCoder-main,ase2025/2502.17139v2.pdf,ase2025/FastCoder-main/datastore/get_repo_datastore.py,ase2025/FastCoder-main/datastore/get_repo_datastore.py,69,72,datastore_path = f'RepoEval_datastore/{model_type}/datastore_repo_{repo}.idx'
find me the supervised fine tuning based on the differneces between correct and incorrect code,ase2025/FGIT-main,ase2025/2503.16913v3.pdf,ase2025/FGIT-main/Train/data.py,ase2025/FGIT-main/Train/data.py,122,129,supervised fine tuning based on differences between correct and incorrect code
how does it build features based on correct and incorrect code,ase2025/FGIT-main,ase2025/2503.16913v3.pdf,ase2025/FGIT-main/Train/data.py,ase2025/FGIT-main/Train/data.py,240,246,building token features with tokenize_code function
does it tokenize the code before feeding or feed the whole code block at the same time?,ase2025/FGIT-main,ase2025/2503.16913v3.pdf,ase2025/FGIT-main/Train/data.py,ase2025/FGIT-main/Train/data.py,239,292,"In the function filter_token_diff, the code only takes one input at a time."
find me the file associated with the training,ase2025/FGIT-main,ase2025/2503.16913v3.pdf,ase2025/FGIT-main/Train/data.py,ase2025/FGIT-main/Train/data.py,4,13,training data
how does it calculate the differences between codes? can you find me those?,ase2025/FGIT-main,ase2025/2503.16913v3.pdf,ase2025/FGIT-main/Train/data.py,ase2025/FGIT-main/Train/data.py,120,179,filter_line_diff function calculates the differences between codes by iterating over each line of input code and checking if a diff corresponds to a specific line number
where is the pruner that the paper mentions to reduce the direction diversion,ase2025/CoSIL-master,ase2025/2503.22424v3.pdf,ase2025/CoSIL-master/get_lite_structure.py,ase2025/CoSIL-master/get_lite_structure.py,12,13,"The code mentions pruning in general context, but it's not specific to direction diversion."
how does it merge the predicted location and where ?,ase2025/CoSIL-master,ase2025/2503.22424v3.pdf,ase2025/CoSIL-master/get_lite_structure.py,ase2025/CoSIL-master/get_lite_structure.py,0,31,the code is trying to merge predicted location and where clause (or table alias as WHERE)
how does it first start processing the bug?,ase2025/CoSIL-master,ase2025/2503.22424v3.pdf,ase2025/CoSIL-master/get_lite_structure.py,ase2025/CoSIL-master/get_lite_structure.py,14,31,first start processing the bug by checking if JSON file exists at ./repo_structures_verified/{instance_id}.json
"in the agentless scenario, how does it retrieve the file location?",ase2025/CoSIL-master,ase2025/2503.22424v3.pdf,ase2025/CoSIL-master/get_lite_structure.py,ase2025/CoSIL-master/get_lite_structure.py,19,22,"The file location is retrieved using the json_file_path variable, which constructs the JSON file path using the instance_id property."
where does it run the regression tests?,ase2025/CoSIL-master,ase2025/2503.22424v3.pdf,ase2025/CoSIL-master/get_verified_structure.py,ase2025/CoSIL-master/get_verified_structure.py,60,119,This file runs the regression tests on verified structures in line 60-119.
it says it can compress long codes using qwen models. find me the code,ase2025/LongCodeZip-main,ase2025/2510.00446v1.pdf,ase2025/LongCodeZip-main/demo.py,ase2025/LongCodeZip-main/demo.py,3,10,This file contains code that uses the Qwen model for long code compression.
the metric mentions perplexity where does it calculate?,ase2025/LongCodeZip-main,ase2025/2510.00446v1.pdf,ase2025/LongCodeZip-main/demo.py,ase2025/LongCodeZip-main/demo.py,40,48,perplexity is calculated in compress_code_file function call of class LongCodeZip
so it divides big file to small chunks mentioned in the paper? can you find it?,ase2025/LongCodeZip-main,ase2025/2510.00446v1.pdf,ase2025/LongCodeZip-main/demo.py,ase2025/LongCodeZip-main/demo.py,16,17,Compresses the code into smaller chunks as mentioned in the paper.
it also retrieves the function using rag based method? can you find it?,ase2025/LongCodeZip-main,ase2025/2510.00446v1.pdf,ase2025/LongCodeZip-main/assets/example_context.py,ase2025/LongCodeZip-main/assets/example_context.py,104,145,_run_llm_step method uses rag based method to retrieve the function
find me the evaluation functions?,ase2025/LongCodeZip-main,ase2025/2510.00446v1.pdf,ase2025/LongCodeZip-main/assets/example_context.py,ase2025/LongCodeZip-main/assets/example_context.py,120,146,"evaluation functions, specifically _finalize_step, execute_task, and run_llm_step"
how does it generate comments for a given code segment?,fse2025/CodeBridge-main,fse2025/2504.07740v1.pdf,fse2025/CodeBridge-main/embedding.py,fse2025/CodeBridge-main/embedding.py,16,23,"model.get_embedding(inputs, is_nl) returns sentence embeddings "
where does it calculate all the metrics for this experiement?,fse2025/CodeBridge-main,fse2025/2504.07740v1.pdf,fse2025/CodeBridge-main/eval.py,fse2025/CodeBridge-main/eval.py,120,179,"calculate MRR, recalls in get_results scores matrix and labeld urls"
it uses model to generate code using dataset? where?,fse2025/CodeBridge-main,fse2025/2504.07740v1.pdf,fse2025/CodeBridge-main/embedding.py,fse2025/CodeBridge-main/embedding.py,60,87,ModelEmbedding class is defined here
it evalutes given the level and code? find me the relevant code sections,fse2025/CodeBridge-main,fse2025/2504.07740v1.pdf,fse2025/CodeBridge-main/embedding.py,fse2025/CodeBridge-main/embedding.py,34,50,defines BGE_Embedding class with init and get_embedding methods
how does it build features given the text?,fse2025/CodeBridge-main,fse2025/2504.07740v1.pdf,,,0,15,
where does it process the patches?,fse2025/Agentless-main,fse2025/2407.01489v2.pdf,fse2025/Agentless-main/classification/graph_classification.py,fse2025/Agentless-main/classification/graph_classification.py,60,119,where it matches exact patch
it creates the github repo like structure in the code to further processing?,fse2025/Agentless-main,fse2025/2407.01489v2.pdf,fse2025/Agentless-main/classification/graph_classification.py,fse2025/Agentless-main/classification/graph_classification.py,59,119,it creates the github repo like structure in the code to further processing
in the paper they mentioned about the costs ? where do they calculate the cost,fse2025/Agentless-main,fse2025/2407.01489v2.pdf,fse2025/Agentless-main/classification/graph_classification.py,fse2025/Agentless-main/classification/graph_classification.py,60,64,"The cost calculation is not present in this snippet. However, the variable 'cost' or any related mention in terms of calculation is absent"
where do they plot the results?,fse2025/Agentless-main,fse2025/2407.01489v2.pdf,fse2025/Agentless-main/classification/graph_classification.py,fse2025/Agentless-main/classification/graph_classification.py,60,69,where do they plot the results?
where do they work with the metadata?,fse2025/Agentless-main,fse2025/2407.01489v2.pdf,fse2025/Agentless-main/classification/load_filtered_benchmark.py,fse2025/Agentless-main/classification/load_filtered_benchmark.py,8,24,reading metadata from csv file