In this section, we present our methodology in answering
our research questions. Figure 1 presents the changes we
made to various aspects of OMG in answering each research
question.
A. Base Commit Context
In addition to the commit diff, we utilized six different
contextual pieces of information about a commit that were
proposed and utilized by OMG [3]. Specifically, for each
commit, we provided the following contextual information to
the OLLM/SLM: 1) Associated issues on the version control
system or issue tracking service 2) Associated pull requests 3)
Relative importance of changed files 4) The software main-
tenance activity type of the commit 5) Multi-Intent Method
Summaries of changed methods [31] 6) Summary of changed
classes. Since the latter three components (4-6) of the commit
context are generated by an LLM, we refer to them as the
LLM-derived commit context. We used this context as the basis
for our study. However, we made specific refinements when
addressing our RQs, which we detail in subsection F.
One of the contextual refinements we propose in this work
is altering how method summaries were generated for the
affected methods. Therefore, it is important to discuss the
original approach adopted by OMG. OMG employs the Multi-
Intent Method Summarization (MMS) technique proposed by
Geng et al. [32] to summarize the methods affected by a
commit, MMS is an LLM-based method comment generation
approach that uses few-shot prompting to generate the sum-
maries for a method from five different aspects (Developerâ€™s
intents) as follows: 1) What describes the functionality of a
method 2) Why Explains the reason why a method is provided
or the design rationale of the method 3) How-to-use Describes
the usage or the expected set-up of using a method 4) How-
it-is-done Describes the implementation details of a method
5) Property Asserts properties of a method including pre-
conditions or post-conditions of a method.
Later in this section (See subsection F), we discuss how
MMS is employed by OMG, why it is not fit for the CMG
task, and how we can overcome its limitations.
B. Datasets
Since our work builds on OMG, our dataset should include
practitioner-evaluated CMs generated by OMG. This ensures
our ground truth CMs have high human evaluation scores [3].
The dataset includes 381 commits from 32 Apache projects in
Java. We used this dataset to compare our CMs generated using
our approach with those generated by OMG using automated
evaluation metrics and practitioner surveys.
Additionally, we used the same dataset to evaluate the
performance of our candidate models in producing LLM-
derived commit context (as discussed earlier in this section).
Specifically, for the software maintenance activity type clas-
sifier, we used the same dataset of 1,151 commits in Java
manually labeled with three maintenance activities (Corrective,
Perfective, and Adaptive). To evaluate the performance of
candidate OLLMs/SLMs in class summarization, similar to
the approach taken by Li et al. [3], due to budget constraints
and the high cost of using GPT-4, we sampled 384 class-
summary pairs (confidence level 95%, margin of error 5%)
from the class summary dataset used by OMG. Lastly, to
examine our candidate OLLMs/SLMs in generating method
summaries, we used the test set utilized by OMG. All these
datasets are provided in the supplementary [33].
C. Evaluation Metrics
1) Automated Metrics: OMG-generated CMs have been
evaluated by practitioners and achieved high human evaluation
scores [3]. Therefore, we postulated that if we make our
CMs similar to those generated by OMG, they would achieve
acceptable results when evaluated by practitioners. This ap-
proach allowed us to use the similarity to OMG-generated
CMs as an initial quality assurance measure before conducting
human evaluations. Accordingly, we used standard evaluation
metrics that are used to compare CMs with state-of-the-art
machine-generated CMs [3], [23], [26]. Specifically, we used
BLEU, METEOR, and ROUGE-L to measure the similarity
between the CMs generated by an OLLM and SLM with
those generated by OMG. Additionally, following the common
practices in CMG research [3], we reported automated metrics
by comparing our CMs with human-written CMs.
2) Human Metrics: In order to evaluate our CMs by practi-
tioners, we opted for using the four human evaluation metrics
proposed by Li et al. in our surveys. These metrics were
developed through careful study of CMG literature and discus-
sions among researchers [3]. The metrics are: 1) Rationality,
which assesses whether a CM provides a logical explanation
for the code change and identifies the software maintenance
activity type. 2) Comprehensiveness, which evaluates whether
the message summarizes what has been changed and includes
relevant important details. 3) Conciseness, which measures
the brevity of a CM. 4) Expressiveness, which examines the
grammatical correctness and fluency of the CM.